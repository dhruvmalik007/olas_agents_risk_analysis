import asyncio
from playwright.async_api import async_playwright
import streamlit as st
import pandas as pd
import plotly.express as px
from fuzzywuzzy import process
from bs4 import BeautifulSoup
import json
import os

class RiskAnalysisFramework:
    def __init__(self):
        self.entries = []

    async def scrape_data(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            await page.goto("https://registry.olas.network/ethereum/agents")

            progress_bar = st.progress(0)
            page_number = 1

            while True:
                st.info(f"Scraping page {page_number}...")
                await page.wait_for_selector('tbody.ant-table-tbody')
                rows = await page.query_selector_all('tbody.ant-table-tbody tr.ant-table-row')
                
                for row in rows:
                    cells = await row.query_selector_all('td')
                    if len(cells) == 5:
                        id_val = await cells[0].inner_text()
                        name_val = await cells[1].inner_text()
                        owner_val = await cells[2].inner_text()
                        hash_val = await cells[3].inner_text()
                        entry = {
                            'id': id_val,
                            'name': name_val,
                            'owner': owner_val,
                            'hash': hash_val,
                            'agent_url': f"https://registry.olas.network/ethereum/agents/{id_val}",
                            'owner_link': f"https://etherscan.io/address/{owner_val.strip()}",
                            'hash_link': f"https://gateway.autonolas.tech/ipfs/{hash_val.strip()}"
                        }
                        self.entries.append(entry)

                next_button = await page.query_selector('li.ant-pagination-next:not(.ant-pagination-disabled) button')
                if next_button:
                    await next_button.click()
                    await page.wait_for_load_state('networkidle')
                    page_number += 1
                    progress_bar.progress(page_number * 10)  # Update progress bar
                else:
                    break

            progress_bar.progress(100)  # Complete progress bar
            st.success("Scraping completed!")
            await browser.close()

            # Load existing data from JSON file
            json_file_path = '/workspaces/olas_agents_risk_analysis/example_scrape_info/agent_status.json'
            if os.path.exists(json_file_path):
                with open(json_file_path, 'r') as file:
                    existing_data = json.load(file)
            else:
                existing_data = []

            # Update existing data with new entries
            existing_ids = {entry['id'] for entry in existing_data}
            new_entries = [entry for entry in self.entries if entry['id'] not in existing_ids]
            updated_data = existing_data + new_entries

            # Save updated data to JSON file
            with open(json_file_path, 'w') as file:
                json.dump(updated_data, file, indent=4)

    def load_data(self):
        json_file_path = '/workspaces/olas_agents_risk_analysis/example_scrape_info/agent_status.json'
        if os.path.exists(json_file_path):
            with open(json_file_path, 'r') as file:
                self.entries = json.load(file)
        else:
            st.warning("agent_status.json not found. Please scrape the agents first.")

    def search_entries(self, query):
        st.info(f"Searching for agents matching: {query}")
        results = [entry for entry in self.entries if query.lower() in entry['name'].lower()]
        return results

    def render_results(self, results):
        if results:
            st.write("### Search Results")
            for result in results:
                st.write(f"üîç **Name:** {result['name']}")
                st.write(f"üë§ **Owner:** {result['owner']}")
                st.write(f"üîó **Hash:** {result['hash']}")
                st.write(f"üîó **Agent URL:** {result['agent_url']}")
                st.write(f"üîó **Owner Link:** {result['owner_link']}")
                st.write(f"üîó **Hash Link:** {result['hash_link']}")
                st.write("---")
            
            # Additional sections
            st.write("### Risk Analysis")

            # Define the data for the 5 sections
            data = {
                'Section': ['UX risk', 'Model risk', 'Agentic memory', 'Metadata analysis', 'Unchain-risk'],
                'Status': ['Critical', 'Medium', 'Perfect', 'Medium', 'Perfect'],
                'Details': [
                    'This section requires immediate attention.',
                    'This section needs to be monitored closely.',
                    'This section is in good condition.',
                    'This section needs to be monitored closely.',
                    'This section is in good condition.'
                ]
            }

            df = pd.DataFrame(data)

            # Create a pie chart using Plotly
            fig = px.pie(df, values='Status', names='Section', title='Section Status',
                         color='Status', color_discrete_map={'Critical':'red', 'Medium':'yellow', 'Perfect':'green'},
                         hover_data=['Details'])

            # Display the chart using Streamlit
            st.plotly_chart(fig)
        else:
            st.warning("No results found.")

